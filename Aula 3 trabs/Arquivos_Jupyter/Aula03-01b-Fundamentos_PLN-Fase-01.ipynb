{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c90ea5-5c38-4d5b-a306-febc709276dd",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural (PLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c69a9-5c6e-44d3-b861-43891038529e",
   "metadata": {},
   "source": [
    "## **1. O que √© Processamento de Linguagem Natural (PLN)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f1296-e2ce-4275-8173-85bfc3b6ec63",
   "metadata": {},
   "source": [
    "### **Defini√ß√£o:**\n",
    "O **Processamento de Linguagem Natural (Natural Language Processing ‚Äì NLP)** √© um ramo da **Intelig√™ncia Artificial** e da **Ci√™ncia da Computa√ß√£o** que se dedica a permitir que os **computadores entendam, interpretem, gerem e respondam a linguagem humana**, seja escrita ou falada.\n",
    "\n",
    "Em outras palavras, o PLN tenta \"ensinar\" aos computadores como interpretar textos ou fala como humanos fazem, reconhecendo significados, inten√ß√µes e emo√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1d56f-7fe7-4b30-bb1e-108b0fafa03d",
   "metadata": {},
   "source": [
    "## **2. Por que o PLN √© Importante?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38d1f7-e17e-45a3-bf1b-6b288eeca5d0",
   "metadata": {},
   "source": [
    "A linguagem natural √© a forma principal de comunica√ß√£o dos seres humanos. Com o avan√ßo das tecnologias digitais, a quantidade de dados em formato textual (como emails, redes sociais, livros, artigos cient√≠ficos) cresce exponencialmente. \n",
    "\n",
    "O PLN permite:\n",
    "\n",
    "- Automatizar tarefas manuais com texto.\n",
    "- Extrair informa√ß√µes √∫teis de grandes volumes de dados textuais.\n",
    "- Criar interfaces mais naturais entre humanos e m√°quinas (ex: assistentes virtuais).\n",
    "- Facilitar tradu√ß√µes autom√°ticas.\n",
    "- Analisar sentimentos, opini√µes e tend√™ncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e25e7-1fa3-4b2f-b2d4-4831c11a1a90",
   "metadata": {},
   "source": [
    "## **3. Aplica√ß√µes do PLN no Cotidiano**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e56d9-1950-4a7f-83a1-d28e5a43fe71",
   "metadata": {},
   "source": [
    "Vamos ver algumas aplica√ß√µes reais do PLN que voc√™ provavelmente j√° utilizou:\n",
    "\n",
    "| Aplica√ß√£o | Descri√ß√£o |\n",
    "|----------|-----------|\n",
    "| **Assistentes Virtuais** (ex: Siri, Alexa, Google Assistant) | Entendem perguntas em linguagem natural e respondem. |\n",
    "| **Tradutores Autom√°ticos** (ex: Google Translate) | Traduz textos entre idiomas diferentes. |\n",
    "| **An√°lise de Sentimento** | Avalia se uma frase √© positiva, negativa ou neutra. |\n",
    "| **Chatbots** | Responde automaticamente a usu√°rios em sites e apps. |\n",
    "| **Classifica√ß√£o de Textos** | Separa textos por categorias (ex: not√≠cias esportivas vs. pol√≠ticas). |\n",
    "| **Sumariza√ß√£o Autom√°tica** | Gera resumos de longos textos. |\n",
    "| **Detec√ß√£o de Pl√°gio** | Identifica c√≥pias de textos. |\n",
    "| **Corre√ß√£o Ortogr√°fica e Gramatical** | Corrige erros em textos. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43079ecf-14e9-410e-9c2d-874e82319f78",
   "metadata": {},
   "source": [
    "## **4. Fundamentos do PLN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89fff5-29aa-4fcd-9b88-0827547306e3",
   "metadata": {},
   "source": [
    "Para que um computador entenda a linguagem natural, ele precisa passar por diversos est√°gios de processamento. Vamos entender cada um desses est√°gios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c4466-01bd-4bc0-821c-738653f74d76",
   "metadata": {},
   "source": [
    "### **4.1 Tokeniza√ß√£o**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de31d58-d058-449f-8005-aaa87fd098c3",
   "metadata": {},
   "source": [
    "**Tokeniza√ß√£o** √© o processo de dividir um texto em unidades menores chamadas *tokens*. Cada token pode ser uma palavra, n√∫mero, pontua√ß√£o, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a37d6-783c-4829-98ca-ff82ae083f88",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo\n",
    "- Carga das bibliotecas\n",
    "- Download de arquivos de apoio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c129bf-c19e-4608-8b2d-731889997559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\nickolas.sa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import regex\n",
    "import unicodedata\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Baixa o modelo de tokeniza√ß√£o para portugu√™s e outros compomentes do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')  # Suporte multil√≠ngue\n",
    "nltk.download('wordnet')   # WordNet principal\n",
    "nltk.download('rslp')\n",
    "\n",
    "#def download_nltk_resource(resource):\n",
    "#    try:\n",
    "#        nltk.data.find(resource)\n",
    "#    except LookupError:\n",
    "#        nltk.download(resource.split('/')[-1])\n",
    "#\n",
    "# Baixa os recursos necess√°rios\n",
    "# download_nltk_resource('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a2432-4994-4669-ba42-cf7b28cf05b5",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713eca1-1234-4a2d-bf47-87ff183a6272",
   "metadata": {},
   "source": [
    "- Importa a biblioteca `NLTK` (**Natural Language Toolkit**), usada para processamento de linguagem natural em Python.\n",
    "- Importa a fun√ß√£o `word_tokenize`, que divide um texto em palavras ou tokens.\n",
    "- Ao rodar esse c√≥digo, ele garante que o recurso punkt esteja instalado, mesmo que ele ainda n√£o tenha sido baixado antes.\n",
    "nltk.download('punkt')\n",
    "\n",
    "1. **`nltk.download('punkt')`**  \n",
    "   - Baixa o **tokenizador \"Punkt\"**, usado para dividir textos em frases e palavras.\n",
    "\n",
    "2. **`nltk.download('punkt_tab')`**  \n",
    "   - Baixa tabelas auxiliares para o tokenizador, melhorando sua efici√™ncia.\n",
    "\n",
    "3. **`nltk.download('stopwords')`**  \n",
    "   - Baixa listas de **palavras irrelevantes (stopwords)** como \"de\", \"e\", \"o\", que s√£o filtradas em pr√©-processamento.\n",
    "\n",
    "4. **`nltk.download('omw-1.4')`**  \n",
    "   - Baixa suporte multil√≠ngue para o **Open Multilingual WordNet**, √∫til para tarefas como l√©xico e sem√¢ntica.\n",
    "\n",
    "5. **`nltk.download('wordnet')`**  \n",
    "   - Baixa o **WordNet** (em ingl√™s), um banco de dados l√©xico usado para sin√¥nimos e an√°lise de significado.\n",
    "\n",
    "6. **`nltk.download('rslp')`**  \n",
    "   - Baixa o **Removedor de Sufixos da L√≠ngua Portuguesa (RSLP)**, usado para **stemming** (reduzir palavras √† raiz, como \"correndo\" ‚Üí \"corr\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3293aa-d5c4-4601-9a8d-ad8ecc258b63",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - Carregando e processando texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d532c8-5680-44cb-a570-2bb2ce0a3017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: \n",
      "\t['Ol√°, tudo bem? Este √© um texto de Exemplo!', 'Eu amo programa√ß√£o em Python e Machine Learning.', 'Texto com MUITAS PONTUA√á√ïES... e alguns STOP WORDS!', 'Outro exemplo: A corrida de dados √© essencial em ML!!!']\n",
      "\n",
      "Texto limpo   : \n",
      "\t['ol√° tudo bem texto exemplo', 'amo programa√ß√£o python machine learning', 'texto muitas pontua√ß√µes alguns stop words', 'outro exemplo corrida dados essencial ml']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Lista de textos de exemplo\n",
    "##\n",
    "#texto = \"Bom dia! Como voc√™ est√°?\"\n",
    "#texto = \"Ol√°, tudo bem? Meu nome √© Z√© das Coves. Gosto de programa√ß√£o!\"\n",
    "texto = [\n",
    "    \"Ol√°, tudo bem? Este √© um texto de Exemplo!\",\n",
    "    \"Eu amo programa√ß√£o em Python e Machine Learning.\",\n",
    "    \"Texto com MUITAS PONTUA√á√ïES... e alguns STOP WORDS!\",\n",
    "    \"Outro exemplo: A corrida de dados √© essencial em ML!!!\"\n",
    "]\n",
    "\n",
    "\n",
    "# 1) Definir stopwords (ex.: portugu√™s, mas ajuste conforme sua necessidade)\n",
    "# \"de\", \"a\", \"o\", \"e\", ...\n",
    "stopwords_pt = set(stopwords.words('portuguese'))  \n",
    "\n",
    "# 2) Fun√ß√£o de limpeza e tokeniza√ß√£o\n",
    "def preprocess_text(text):\n",
    "    # a) Colocar tudo em min√∫sculo\n",
    "    text = text.lower()\n",
    "    \n",
    "    # b) Remover pontua√ß√µes e caracteres especiais (regex)\n",
    "    #text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    #text = re.sub(r'[.,!?;:()\\[\\]{}\\'\"\\-_]', '', text)\n",
    "    text = regex.sub(r'\\p{P}+', '', text)\n",
    "\n",
    "    # c) Tokenizar de forma simples (split por espa√ßo)\n",
    "    #tokens = text.split()\n",
    "    tokens = word_tokenize(text, language='portuguese')    \n",
    "    \n",
    "    # d) Remover stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords_pt]\n",
    "    \n",
    "    # e) Reunir tokens novamente, se quisermos gerar um \"texto limpo\"\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# 3) Limpar cada texto na lista\n",
    "texto_limpo = [preprocess_text(txt) for txt in texto]\n",
    "\n",
    "\n",
    "print (f\"Texto original: \\n\\t{texto}\\n\")\n",
    "print (f\"Texto limpo   : \\n\\t{texto_limpo}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6f096-d03c-4ea7-88e7-6650ae4d8155",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcad25-626c-42d0-b746-2e3b10844568",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b388f367-c0ad-4b11-b132-95241e322b58",
   "metadata": {},
   "source": [
    "### **4.2 Lemmatiza√ß√£o e Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32811368-cc12-4c0c-93c8-fd1c84724e07",
   "metadata": {},
   "source": [
    "Esses processos transformam palavras em suas formas base ou raiz.\n",
    "\n",
    "- **Stemming**: Reduz palavras √† sua raiz, sem garantir que seja uma palavra real.\n",
    "  - Ex: \"running\" ‚Üí \"run\"\n",
    "    \n",
    "- **Lemmatization**: Reduz palavras √† sua forma can√¥nica (lemma), considerando o contexto gramatical.\n",
    "  - Ex: \"running\" ‚Üí \"running\"\n",
    "    \n",
    "Este c√≥digo importa bibliotecas e ferramentas essenciais para **pr√©-processamento de texto** em PLN (Processamento de Linguagem Natural), com foco em **redu√ß√£o de palavras √†s suas formas base** (stemming e lematiza√ß√£o).  \n",
    "\n",
    "**1. Stemmers (Redu√ß√£o Radical)**  \n",
    "- **`PorterStemmer`** (NLTK):  \n",
    "  - Algoritmo para **stemming em ingl√™s** (ex: \"running\" ‚Üí \"run\").  \n",
    "- **`RSLPStemmer`** (NLTK):  \n",
    "  - Algoritmo para **stemming em portugu√™s** (ex: \"correndo\" ‚Üí \"corr\").  \n",
    "\n",
    "**2. Lematiza√ß√£o (Redu√ß√£o √† Forma Can√¥nica)**  \n",
    "- **`WordNetLemmatizer`** (NLTK):  \n",
    "  - Usa o **WordNet** para lematiza√ß√£o em ingl√™s (ex: \"better\" ‚Üí \"good\").  \n",
    "  - *Requer `nltk.download('wordnet')`*.  \n",
    "\n",
    "**3. spaCy (Processamento Avan√ßado)**  \n",
    "- **`spacy`**:  \n",
    "  - Biblioteca poderosa para PLN, com suporte a **lematiza√ß√£o, POS tagging (an√°lise gramatical) e NER (reconhecimento de entidades)**.  \n",
    "  - *Para portugu√™s, normalmente carrega-se o modelo `pt_core_news_sm`*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb3c4d-e5e7-4016-97fd-4b19514c0a91",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - para ingl√™s\n",
    "‚≠êÔ∏è **O c√≥digo possui suporte a ingl√™s, mas que n√£o funciona muito bem para portugu√™s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17cad2c-97b9-4845-b491-fc0a13f0c392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming    : run\n",
      "Lematiza√ß√£o : running\n",
      "\n",
      "Stemming    : correndo\n",
      "Lematiza√ß√£o : correndo\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, RSLPStemmer\n",
    "import spacy\n",
    "\n",
    "##\n",
    "# Ingl√™s\n",
    "##\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\nStemming    :\", stemmer.stem(\"running\"))\n",
    "print(\"Lematiza√ß√£o :\", lemmatizer.lemmatize(\"running\"))\n",
    "\n",
    "print(\"\\nStemming    :\", stemmer.stem(\"correndo\"))\n",
    "print(\"Lematiza√ß√£o :\", lemmatizer.lemmatize(\"correndo\"))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9655695",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - para portugu√™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2bd421-48f6-48a3-8bdd-52126cc32754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras    : ['correr', 'correndo', 'corrida', 'c√£o', 'c√£es']\n",
      "Stemming PT : ['corr', 'corr', 'corr', 'c√£o', 'c√£o'] \n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStemming PT : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpalavras_stemmed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Portugues - Lematiza√ß√£o \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m##\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Carrega o modelo em portugu√™s\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     20\u001b[0m frase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEu estava correndo com meus c√£es na praia\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(frase)\n",
      "File \u001b[1;32mc:\\_mlUc1\\mlEnv\\lib\\site-packages\\spacy\\__init__.py:52\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     29\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     36\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\_mlUc1\\mlEnv\\lib\\site-packages\\spacy\\util.py:484\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "##\n",
    "# Portugues - Stemming\n",
    "##\n",
    "stemmer_pt = RSLPStemmer()\n",
    "\n",
    "palavras = [\"correr\", \"correndo\", \"corrida\", \"c√£o\", \"c√£es\"]\n",
    "print(f\"\\nPalavras    : {palavras}\")\n",
    "\n",
    "palavras_stemmed = [stemmer_pt.stem(p) for p in palavras]\n",
    "print(f\"Stemming PT : {palavras_stemmed} \\n\")\n",
    "\n",
    "\n",
    "##\n",
    "# Portugues - Lematiza√ß√£o \n",
    "##\n",
    "\n",
    "# Carrega o modelo em portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")  \n",
    "\n",
    "frase = \"Eu estava correndo com meus c√£es na praia\"\n",
    "doc = nlp(frase)\n",
    "for token in doc:\n",
    "    print(f\"Palavra: {token.text} \\t ‚Üí Lemma: {token.lemma_}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564155bd-a8a3-4244-b4ba-9b318234e1ab",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ef115f-3606-4f18-9416-1dea39357463",
   "metadata": {},
   "source": [
    "Por que o `WordNetLemmatizer` n√£o funciona para portugu√™s?\n",
    "- O WordNetLemmatizer depende do WordNet, que s√≥ existe para ingl√™s.\n",
    "- N√£o h√° um equivalente oficial do WordNet para portugu√™s no NLTK.\n",
    "- Tentar us√°-lo resultar√° em erros ou lemmatiza√ß√£o incorreta.\n",
    "\n",
    "Alternativa com NLTK (Stemming, mas n√£o Lemmatiza√ß√£o)\n",
    "- Se voc√™ precisa usar o NLTK e n√£o pode instalar spaCy/Stanza, pode usar o RSLPStemmer para stemming (mas lembre-se: stemming ‚â† lemmatiza√ß√£o)\n",
    "- Limita√ß√£o: O stemming corta sufixos sem garantir que a palavra resultante exista no dicion√°rio (ex: \"c√£es\" ‚Üí \"c√£\").\n",
    "\n",
    "**Conclus√£o**\n",
    "- ‚úÖ **Para lemmatiza√ß√£o em portugu√™s, use spaCy ou Stanza** (eles t√™m modelos pr√©-treinados para PT).\n",
    "- ‚ùå **N√£o use `WordNetLemmatizer` para portugu√™s** (n√£o funciona).\n",
    "- ‚ö†Ô∏è **Se precisar apenas de stemming, use `RSLPStemmer` do NLTK**, mas lembre-se que os resultados s√£o menos precisos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf337406-2acf-47ed-950f-3ee27fa8970b",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c01ae-e5d1-409d-8ace-282e82ba9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Portugues\n",
    "##\n",
    "\n",
    "# Carrega o modelo em portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")  \n",
    "\n",
    "def lemmatizar_frases(lista_frases):\n",
    "    frases_lematizadas = []\n",
    "    \n",
    "    for frase in lista_frases:\n",
    "        doc = nlp(frase)\n",
    "        # Lematiza cada token e junta novamente em uma string\n",
    "        frase_lematizada = \" \".join([token.lemma_ for token in doc])\n",
    "        frases_lematizadas.append(frase_lematizada)\n",
    "    \n",
    "    return frases_lematizadas\n",
    "\n",
    "def remover_acentos(texto):\n",
    "    # Normaliza o texto (decomp√µe os caracteres acentuados em caracteres simples + acento)\n",
    "    texto_normalizado = unicodedata.normalize('NFKD', texto)\n",
    "\n",
    "    # Remove os caracteres de acentua√ß√£o (como ~, ¬¥, `, ^, etc.)\n",
    "    texto_sem_acentos = ''.join(c for c in texto_normalizado if not unicodedata.combining(c))\n",
    "    \n",
    "    return texto_sem_acentos\n",
    "\n",
    "\n",
    "# Aplica a lematiza√ß√£o\n",
    "texto_lematizado = lemmatizar_frases(texto_limpo)\n",
    "\n",
    "# Removendo os acentos\n",
    "texto_lematizado_sem_acento=[]\n",
    "for linha in texto_lematizado:\n",
    "    linha_sem_acento=remover_acentos(linha)\n",
    "    texto_lematizado_sem_acento.append(linha_sem_acento)\n",
    "\n",
    "# Mostra o resultado\n",
    "for original, lematizada, lematizada_sem_acento in zip(texto_limpo, texto_lematizado,  texto_lematizado_sem_acento):\n",
    "    print(f\"Original             : {original}\")\n",
    "    print(f\"Lematizada           : {lematizada}\")\n",
    "    print(f\"Lematizada s/acento  : {lematizada_sem_acento}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc0602-9104-4d22-aecd-199716addfae",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abe037-76f9-4d50-aada-873f7d82a39f",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122925e",
   "metadata": {},
   "source": [
    "### **4.3 Vetoriza√ß√£o do texto**\n",
    "\n",
    "Computadores trabalham com n√∫meros, ent√£o precisamos representar palavras como vetores num√©ricos.\n",
    "\n",
    "**T√©cnicas Comuns:**\n",
    "- **Bag of Words (BoW)**: Conta a frequ√™ncia de cada palavra.\n",
    "- **TF-IDF**: Considera a import√¢ncia relativa de uma palavra em um documento.\n",
    "- **Word Embeddings**: Representa√ß√µes densas de palavras aprendidas com redes neurais (ex: Word2Vec, GloVe, FastText)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99174b97-1856-416a-be5b-d5adb7c8467d",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo \n",
    "- Vetorizar com CountVectorizer (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdc714-4852-47d1-9e54-57583e199ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Vetorizar com CountVectorizer (Bag-of-Words)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(texto_lematizado_sem_acento)\n",
    "\n",
    "print(f\"\\n\\nVocabul√°rio: \\n\\n{vectorizer.get_feature_names_out()}\\n\")\n",
    "print(f\"\\nMatriz BOW:\\n\\n {bow_matrix.toarray()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994ad3b-b051-4834-b819-b5bdcc474d06",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9d0d6-7b5d-4f1f-8581-dad64e66d141",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018a1a0-f9c5-43fe-b1d6-267853be27ec",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - Converter para DataFrame, apenas para visualiza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fceba0-cccc-4bff-82f9-855a09491d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Converter para DataFrame, apenas para visualiza√ß√£o\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "bow_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d066c11-1533-4a20-b385-28618b99ec08",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo  - Salvando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39045218-da15-42a2-ab28-83cf957bf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df.to_csv('./dataset/dataset_bow_matrizx-2025.06.30.csv', index=False)\n",
    "bow_df.to_csv('./dataset/dataset_bow_matrizx-2025.06.30.csv.gzip', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c9edc-97a0-4c73-8ed7-13d5f6b68d60",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - Visualizando o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669e0a1-a192-452e-8719-4f29b852ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (f\"Texto original: \\n\\t{texto}\\n\")\n",
    "print(\"Textos originais:\")\n",
    "for i, t in enumerate(texto):\n",
    "    print(f\"\\t{i+1}: {t}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTextos p√≥s-limpeza:\")\n",
    "for i, ct in enumerate(texto_lematizado_sem_acento):\n",
    "    print(f\"\\t{i+1}: {ct}\")\n",
    "\n",
    "print(\"\\nMatriz Bag-of-Words:\")\n",
    "bow_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1945e-9e47-4a23-b999-7ed4637ee0f2",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1add07e1-1caa-4c22-be2d-a44a2a6930d9",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186329b-4ae7-4dee-94f4-8a0b16d88f9f",
   "metadata": {},
   "source": [
    "### **4.4 An√°lise Sint√°tica (Parsing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436ded3-08c0-47ec-9c0c-250ff995c209",
   "metadata": {},
   "source": [
    "√â o processo de analisar a estrutura gramatical de uma frase para entender a rela√ß√£o entre as palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569ce6c-0e25-4492-9e78-10cb58e3f706",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - Ingl√™s\n",
    "- O c√≥digo funciona bem para ingl√™s mas nem sempre funciona muito bem para portugu√™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72a0df-3113-42ed-bb04-882c1b2bdbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "##\n",
    "# Ingl√™s\n",
    "##\n",
    "frase = \"O gato dorme no sof√°.\"\n",
    "\n",
    "# Separa a frase em tokens\n",
    "tokens = word_tokenize(frase)\n",
    "\n",
    "# Realiza a an√°lise sint√°tica \n",
    "tags = pos_tag(tokens)\n",
    "print(f\"\\n\\nTAGS: \\n\\t {tags} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f095c-0dce-491b-942f-1c54a8d85d12",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f894eb2-a1c6-4dd5-9f70-2b557e5d34d7",
   "metadata": {},
   "source": [
    "Cada par mostra a palavra e seu **`part-of-speech` (POS)** (tag sint√°tica):\n",
    "- `DT`: Determinante\n",
    "- `NN`: Substantivo\n",
    "- `VBZ`: Verbo (presente)\n",
    "- `IN`: Preposi√ß√£o\n",
    "\n",
    "**OBS**:\n",
    "- O `averaged_perceptron_tagger` do NLTK √© um modelo pr√©-treinado **exclusivamente para ingl√™s** e n√£o funciona para portugu√™s.\n",
    "- Se voc√™ precisa de `POS tagging` (etiquetagem gramatical) em portugu√™s, ter√° que usar outras ferramentas, como `spaCy`, `Stanza` ou treinar um modelo pr√≥prio no NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe0fff-7f6e-44ca-ac5f-681e2ac6da5d",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo - Portugu√™s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963ad09-dedd-47ba-ac04-e70115b7394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "##\n",
    "# Portugu√™s\n",
    "##\n",
    "\n",
    "# Modelo para portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")  \n",
    "\n",
    "#texto = \"Eu gosto de programar em Python.\"\n",
    "texto = \"O gato dorme no sof√°.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} ‚Üí {token.pos_}\")  # POS tag (universal)\n",
    "    #print(f\"{token.text} ‚Üí {token.tag_}\")  # Tag detalhada (p.ex., 'VERB' vs. 'VERB__Mood=Ind')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffd682-06bc-43f2-ac89-a351a9ccb45c",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df083099-68e6-43f6-80ec-0057efd46c36",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602691cd-aefb-417e-b3ab-2ad39da27e1c",
   "metadata": {},
   "source": [
    "### [DUP] **4.5 Representa√ß√£o Vetorial de Palavras**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a98134-648a-45ca-8c4a-682e502993d0",
   "metadata": {},
   "source": [
    "Computadores trabalham com n√∫meros, ent√£o precisamos representar palavras como vetores num√©ricos.\n",
    "\n",
    "**T√©cnicas Comuns:**\n",
    "- **Bag of Words (BoW)**: Conta a frequ√™ncia de cada palavra.\n",
    "- **TF-IDF**: Considera a import√¢ncia relativa de uma palavra em um documento.\n",
    "- **Word Embeddings**: Representa√ß√µes densas de palavras aprendidas com redes neurais (ex: Word2Vec, GloVe, FastText)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c89860-23f4-46b2-aea5-10d5ca3e2af0",
   "metadata": {},
   "source": [
    "#### üêç C√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cbe1a-24c6-4882-bdca-a2854555bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"Gosto de programar em Python.\",\n",
    "    \"Python √© uma linguagem poderosa.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1b80c-f509-419d-b7b7-3549d83a1554",
   "metadata": {},
   "source": [
    "#### üîñ Explica√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d590e03-c5a6-444c-a7dd-cb641a96fc46",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML (venv)",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
